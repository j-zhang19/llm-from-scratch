{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f242e9b-9900-48c6-a6b3-8ccf71edd721",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4821617-6c49-433b-bd8d-2c7331e03811",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "if not os.path.isfile(file_path):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "    print(\"Downloading data :\", file_path, \"\\nat :\", url)\n",
    "    urllib.request.urlretrieve(url, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a1a4fb-b9c5-4ea7-a4ec-772eef08b77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters : 20479\n",
      "type : <class 'str'>\n",
      "sample: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters :\", len(raw_text))\n",
    "print(\"type :\", type(raw_text))\n",
    "print(\"sample:\", raw_text[:86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654b1736-8cae-4c81-be31-088c725f5727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690 ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [i.strip() for i in preprocessed if i.strip()]\n",
    "print(len(preprocessed), preprocessed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6380f9f7-81ac-4ca9-9aba-d8245edfa5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5085369-d178-41ff-9fd4-4bd5414c2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {str: id for id, str in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee7a42d-0092-4aa9-ae71-23ec58cae0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab: dict[str, int]):\n",
    "        self.str_to_int: dict[str, int] = vocab\n",
    "        self.int_to_str: dict[int, str] = {id: str for str, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [i.strip() for i in preprocessed if i.strip()]\n",
    "\n",
    "        preprocessed = [i if i in self.str_to_int else \"<|unk|>\" for i in preprocessed]\n",
    "        \n",
    "        ids = [self.str_to_int[str] for str in preprocessed]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: list[int]):\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c15f90bf-01d0-4362-b737-66578e04b592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great surprise to me to hear that, in the height of'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "text = \"great surprise to me to hear that, in the height of\"\n",
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b473c889-cbf3-4d9f-ae0f-ee091167a81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token: id for id, token in enumerate(all_tokens)}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35e11428-bb20-4ddb-817c-9f787dd1169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello.<|endoftext|>Cool dog.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Hello.\"    # unseen word\n",
    "text2 = \"Cool dog.\" # unseen words\n",
    "text = \"<|endoftext|>\".join([text1, text2])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a9ed0b3-d6a5-499d-8a80-30b798eee2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 7, 1131, 1131, 7] -> <|unk|>. <|unk|> <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "print(tokenizer.encode(text), \"->\", tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46de1b6-8f32-4da7-b9f0-4cbd20cbd4f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5b9c0d2-71ea-42b6-a63e-3ce838c538af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "532cd665-8364-4120-be85-4813ad44dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2181c87-d6f1-473a-bd2f-91c28861eaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Hello.<|endoftext|>Cool dog.\n",
      "encoded tokens: [15496, 13, 50256, 34530, 3290, 13]\n",
      "decoded tokens: Hello.<|endoftext|>Cool dog.\n"
     ]
    }
   ],
   "source": [
    "print(\"text:\", text)\n",
    "tokens = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(\"encoded tokens:\", tokens)\n",
    "strings = tokenizer.decode(tokens)\n",
    "print(\"decoded tokens:\", strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9db55bcd-0d39-45bc-88ff-1e84f63171f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Example for an unkown word, it decomposes the word into subwords\n",
      "[33901, 86, 343, 86, 220, 959]\n",
      "33901 Ak\n",
      "86 w\n",
      "343 ir\n",
      "86 w\n",
      "220  \n",
      "959 ier\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "print(\"- Example for an unkown word, it decomposes the word into subwords\")\n",
    "print(tokenizer.encode(\"Akwirw ier\"))\n",
    "for i in tokenizer.encode(\"Akwirw ier\"):\n",
    "    print(i, tokenizer.decode([i]))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Akwirw ier\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c444d2c-d4c4-4769-bfe9-62131c1878dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82e1ef68-f0eb-4f2c-81d5-cd54b1d73799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32068151-ae72-458e-8b28-c77c6d214dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb9de249-5904-4ed9-8695-4099c8ca158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[290, 4920, 2241, 287]\n",
      "y     =[4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f'{x=}')\n",
    "print(f'{y     =}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "958b600a-c7d0-4575-8099-16f886015aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input] ---> [output]\n",
      " and --->  established\n",
      " and established --->  himself\n",
      " and established himself --->  in\n",
      " and established himself in --->  a\n"
     ]
    }
   ],
   "source": [
    "print('[input] ---> [output]')\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"--->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8690c87-a1f1-4a62-9b29-bd4baf201120",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9eea2126-47d7-4f38-951e-ab5b027f2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d9d13270-b2c1-41f2-94d2-0bed11c6b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i+1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "52fde90e-9d4e-42a5-b242-5aa8058ecd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, max_length=256,\n",
    "                     stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a52835f-00cf-43d2-8290-0907417a943b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader(raw_text, batch_size=1, max_length=4,\n",
    "                     stride=1, shuffle=False, drop_last=True, num_workers=0)\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3952738b-9809-42c0-9134-50773eb4aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# with a larger batch_size\n",
    "dataloader = create_dataloader(raw_text, batch_size=8, max_length=4,\n",
    "                     stride=4, shuffle=False, drop_last=True, num_workers=0)\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Targets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419dae8-bab1-4df3-bb47-8ec99838d177",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d1766-a38c-4762-a9dc-2d864171505e",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e1ed5565-bc73-4b14-9390-9764aa6e2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3 # higher value -> more precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fbb6aa99-dc22-41ca-93fe-138acc263075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(6, 3) Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer, embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fdbc0616-e425-4b55-95ef-8a9417aa481a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding vector for token_id=3:\n",
      " tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding vector for token_id=3:\\n\", embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fbb67a48-cf46-41df-81b4-27a87a347e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding vectors for tokens=[2,3,5,1]:\n",
      " tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding vectors for tokens=[2,3,5,1]:\\n\", embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3cbc4-469b-462f-8531-0a80ca323170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b6c80e08-5704-4135-a5fe-a7429f5c4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab # 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4f25a006-5e3a-4724-9528-6a735fb4419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "max_length = 4\n",
    "dataloader = create_dataloader(raw_text, batch_size=batch_size, max_length=max_length,\n",
    "                     stride=max_length, shuffle=False, drop_last=True, num_workers=0)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "be9ea100-fa60-418d-a04e-6060f80c8596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embedding = token_embedding_layer(inputs)\n",
    "print(token_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9ca6386d-e68d-4e3a-894d-db1665e8603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc576a9c-6ba2-4069-b686-c8941b133056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
