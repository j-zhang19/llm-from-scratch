{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f242e9b-9900-48c6-a6b3-8ccf71edd721",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4821617-6c49-433b-bd8d-2c7331e03811",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "if not os.path.isfile(file_path):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "    print(\"Downloading data :\", file_path, \"\\nat :\", url)\n",
    "    urllib.request.urlretrieve(url, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a1a4fb-b9c5-4ea7-a4ec-772eef08b77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters : 20479\n",
      "type : <class 'str'>\n",
      "sample: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters :\", len(raw_text))\n",
    "print(\"type :\", type(raw_text))\n",
    "print(\"sample:\", raw_text[:86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654b1736-8cae-4c81-be31-088c725f5727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690 ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [i.strip() for i in preprocessed if i.strip()]\n",
    "print(len(preprocessed), preprocessed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6380f9f7-81ac-4ca9-9aba-d8245edfa5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5085369-d178-41ff-9fd4-4bd5414c2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {str: id for id, str in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dee7a42d-0092-4aa9-ae71-23ec58cae0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab: dict[str, int]):\n",
    "        self.str_to_int: dict[str, int] = vocab\n",
    "        self.int_to_str: dict[int, str] = {id: str for str, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [i.strip() for i in preprocessed if i.strip()]\n",
    "\n",
    "        preprocessed = [i if i in self.str_to_int else \"<|unk|>\" for i in preprocessed]\n",
    "        \n",
    "        ids = [self.str_to_int[str] for str in preprocessed]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: list[int]):\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c15f90bf-01d0-4362-b737-66578e04b592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great surprise to me to hear that, in the height of'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "text = \"great surprise to me to hear that, in the height of\"\n",
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b473c889-cbf3-4d9f-ae0f-ee091167a81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token: id for id, token in enumerate(all_tokens)}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35e11428-bb20-4ddb-817c-9f787dd1169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello. <|endoftext|> Cool dog.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Hello.\"    # unseen word\n",
    "text2 = \"Cool dog.\" # unseen words\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a9ed0b3-d6a5-499d-8a80-30b798eee2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 7, 1130, 1131, 1131, 7] -> <|unk|>. <|endoftext|> <|unk|> <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "print(tokenizer.encode(text), \"->\", tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbfa97d-5792-4d5c-81fa-bb137a76b4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
